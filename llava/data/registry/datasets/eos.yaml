---
orand-b:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/ORAND-CAR-B_train.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/yuxian_data/images/ORAND-CAR-B

wordart:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/WordArt_train_label_out_processed.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/WordArt

unichart:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/unichart-qa-data/unichart-qa_processed.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/unichart-qa-data/images

ureaderkg:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/ureader_kg/ureader_kg_processed.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/ureader_kg

pathvqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pathvqa/pvqa/train_32k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pathvqa/pvqa/images

medvqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/medvqa/train_data_17k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/medvqa/data/

slake:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/slake/slake_train_val_instruct_high_freq_4.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/slake/imgs

sroie:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/sroie/SROIE_processed.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/sroie/train

llave_onevision_images_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/llava_onevision_sft_images_only_non_repeat_train@fix_relpath.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/images

# 154,608
olmo_pointing:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 27,856
olmo_pointing_qa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing_qa.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 65,934
olmo_pointing_high_freq:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing_high_frequency.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 46,518
olmo_doc_table:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_tables.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 71,282
olmo_doc_doc:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_documents.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 16,551
olmo_doc_diagrams:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_diagrams.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 116,814
olmo_doc_charts:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_charts.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 203,559
olmo_clock200k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/clocks_processed_200k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 71,172
olmo_askanything:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/ask_model_anything.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 162,855
olmo_capqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/capqa.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo

sharegpt4v_gpt4_100k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/ShareGPT4V/jason-filter-sharegpt4v_instruct_gpt4-vision_cap100k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/ShareGPT4V/data

synthdog_en:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/synthdog_en.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/synthdog-en

llava_instruct:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/llava_instruct_150k_zh.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/coco

geoqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/geoqa+.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/geoqa+

ai2d_train_12k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/ai2d_train_12k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/ai2d

text_flan_1m:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/text_flan_1m/data.json
    no_system_prompt: true

shot2story_shotonly:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/shot2story/train-shortclip-processed-bin.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/shot2story/Shot2Story/data/videos_extracted

scienceqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/evaluation/scienceqa/scienceqa_train_12k.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/evaluation/scienceqa/images

sharegpt4v_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/ShareGPT4V/jason-filter-sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/ShareGPT4V/data

########################### vflan ###########################
captioning_image-paragraph-captioning_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_image-paragraph-captioning_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_image-paragraph-captioning_train/images
    no_system_prompt: true

captioning_msrvtt_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_msrvtt_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_msrvtt_train/images
    no_system_prompt: true

captioning_textcap_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_textcap_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/captioning_textcap_train/images
    no_system_prompt: true

chartqa_train_18k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/chartqa_train_18k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/data/chartqa

docvqa_train_10k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/docvqa_train_10k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/data/docvqa

dvqa_train_200k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/dvqa_train_200k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/InternVL/internvl_chat/playground/data/dvqa

generation_visual-dialog_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/generation_visual-dialog_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/generation_visual-dialog_train/images
    no_system_prompt: true

llava-video/academic-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/academic-cap.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/academic-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/academic-mc.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/academic-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/academic-oe.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/activitynetqa-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/activitynetqa-oe.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/nextqa-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/nextqa-mc.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/nextqa-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/nextqa-oe.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/perceptiontest-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/perceptiontest-mc.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/sharegptvideo-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/sharegptvideo-cap.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/sharegpt-video/videos

llava-video/sharegptvideo-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/sharegptvideo-oe.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/sharegpt-video/videos

llava-video/youtube-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/youtube-cap.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/youtube-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/youtube-mc.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

llava-video/youtube-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/datasets/llava-video/youtube-oe.json
    media_dir: /lustre/fsw/nvr_elm_llm/datasets/llava-video/videos

reasoning_clevr_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_clevr_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_clevr_train/images
    no_system_prompt: true

reasoning_nlvr_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_nlvr_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_nlvr_train/images
    no_system_prompt: true

reasoning_visual-mrc_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_visual-mrc_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/reasoning_visual-mrc_train/images
    no_system_prompt: true

vqa_activitynet-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_activitynet-qa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_activitynet-qa_train/images
    no_system_prompt: true

vqa_docvqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_docvqa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_docvqa_train/images
    no_system_prompt: true

vqa_gqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_gqa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_gqa_train/images
    no_system_prompt: true

vqa_ivqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_ivqa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_ivqa_train/images
    no_system_prompt: true

vqa_msrvtt-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_msrvtt-qa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_msrvtt-qa_train/images
    no_system_prompt: true

vqa_msvd-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_msvd-qa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_msvd-qa_train/images
    no_system_prompt: true

vqa_ocr-vqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_ocr-vqa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_ocr-vqa_train/images
    no_system_prompt: true

vqa_st-vqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_st-vqa_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_st-vqa_train/images
    no_system_prompt: true

vqa_viquae_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_viquae_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_viquae_train/images
    no_system_prompt: true

vqa_vqa-v2_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_vqa-v2_train/data.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vflan/vqa_vqa-v2_train/images
    no_system_prompt: true

llava_1_5_mm_align:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/LLaVA-CC3M-Pretrain-595K/chat.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/LLaVA-CC3M-Pretrain-595K/images

allava_caption_vflan:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/ALLaVA-4V/ALLaVA-Caption-VFLAN-4V.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/ALLaVA-4V/

docmatix_750k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_750k.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix

docmatix_2:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_750000-1000000.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix

docmatix_3:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_1000000-1250000.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/Docmatix

pdfa-1m:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/pdfa/vanilla_qa/pdfa_1m.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-pretrain/pdfa/vanilla_qa

unichart-pretrain-1m:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/unichart-pretrain-data/unichart-pretrain_processed_1m.jsonl
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/UniChart-pretrain-images/content/images

mtwi:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/mtwi_train/mtwi_processed.jsonl
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/mtwi_train/image_train

art1:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ART/ART_processed1.jsonl
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ART/train_images

art2:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ART/ART_processed2.jsonl
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ART/train_task2_images

lsvt:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/LSVT/LSVT_processed.jsonl
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/LSVT/train_full_images

sharegpt4v_pretrain:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/jason-filter-share-captioner_coco_lcs_sam_1246k_1107.json
    media_dir: /lustre/fsw/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/data