---
tabmwp_cot:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/workspace/PromptPG/data/tabmwp/problems_train_vila_cot.json
    media_dir: /home/ligengz/workspace/PromptPG/data/tabmwp

tabmwp:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/workspace/PromptPG/data/tabmwp/problems_train_vila.json
    media_dir: /home/ligengz/workspace/PromptPG/data/tabmwp

activitynet-dvc:
    _target_: llava.data.DVCDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/train.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

activitynet-el:
    _target_: llava.data.ELDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/train.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

activitynet-rtl/val:
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/temporal_reasoning/annot_val_1_q229.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

activitynet-rtl:
    _target_: llava.data.RTLDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/temporal_reasoning/activitynet_train_gpt-4-0613_temp_6_f10009.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

ai2d_train_12k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/ai2d_train_12k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/ai2d

captioning_image-paragraph-captioning_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/captioning_image-paragraph-captioning_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/captioning_image-paragraph-captioning_train/images
    no_system_prompt: true

captioning_msrvtt_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/captioning_msrvtt_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/captioning_msrvtt_train/images
    no_system_prompt: true

captioning_textcap_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/captioning_textcap_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/captioning_textcap_train/images
    no_system_prompt: true

charades-el:
    _target_: llava.data.ELDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/charades/train_processed.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/charades/Charades_v1_480

chartqa_train_18k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/chartqa_train_18k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/chartqa

didemo-el:
    _target_: llava.data.ELDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/didemo/train_processed.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/didemo/train_videos_480p

docvqa_train_10k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/docvqa_train_10k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/docvqa

dvqa_train_200k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/dvqa_train_200k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/dvqa

generation_visual-dialog_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/generation_visual-dialog_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/generation_visual-dialog_train/images
    no_system_prompt: true

geoqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/geoqa+.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/geoqa+

llava-instruct-150k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/LLaVA-Instruct-150K/llava_v1_5_mix665k.json
    media_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets

llava_instruct:
    _target_: llava.data.LLaVADataset
    data_path: /home/jasonlu/workspace/InternVL/internvl_chat/playground/llava_instruct_150k_zh.jsonl
    media_dir: /home/jasonlu/workspace/InternVL/internvl_chat/playground/data/coco

llava-video/academic-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/academic-cap.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/academic-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/academic-mc.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/academic-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/academic-oe.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/activitynetqa-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/activitynetqa-oe.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/nextqa-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/nextqa-mc.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/nextqa-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/nextqa-oe.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/perceptiontest-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/perceptiontest-mc.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/sharegptvideo-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/sharegptvideo-cap.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/sharegpt-video/videos

llava-video/sharegptvideo-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/sharegptvideo-oe.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/sharegpt-video/videos

llava-video/youtube-cap:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/youtube-cap.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/youtube-mc:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/youtube-mc.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

llava-video/youtube-oe:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/youtube-oe.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/llava-video/videos

m3it/fm-iqa/train:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/m3it/fm-iqa/train.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/m3it/fm-iqa/images

m3it/fm-iqa/val:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/m3it/fm-iqa/val.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/m3it/fm-iqa/images

m4-instruct-image:
    _target_: llava.data.LLaVANextDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data/m4_instruct_annotations.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data

m4-instruct-image-nuscenes:
    _target_: llava.data.LLaVANextDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data/ligeng_split/nuscenes.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data

m4-instruct-video:
    _target_: llava.data.LLaVANextVideoDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data/m4_instruct_video_filtered.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/M4-Instruct-Data/train_video_and_instruction/extracted-jason

math:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/math/data.json

medical-dvc:
    _target_: llava.data.DVCDataset
    data_path: /home/shijial/workspace/LITA-1.5/data/medical/train.json
    video_dir: /home/shijial/workspace/LITA-1.5/data/medical/videos

medical-el:
    _target_: llava.data.ELDataset
    data_path: /home/shijial/workspace/LITA-1.5/data/medical/train.json
    video_dir: /home/shijial/workspace/LITA-1.5/data/medical/videos

mint-1t-arxiv-captions:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/mint-1t-arxiv/captions-20240904.json
    media_dir: /home/zhijianl/workspace/datasets/mint-1t-arxiv/tiff

nextqa:
    _target_: llava.data.VideoQADataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/nextqa/train_processed.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/nextqa/NExTVideo
    task_prompt: Answer the question using a short phrase.

real-cqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/real-cqa/data.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/datasets/real-cqa/images

reasoning_clevr_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/reasoning_clevr_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/reasoning_clevr_train/images
    no_system_prompt: true

reasoning_nlvr_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/reasoning_nlvr_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/reasoning_nlvr_train/images
    no_system_prompt: true

reasoning_visual-mrc_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/reasoning_visual-mrc_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/reasoning_visual-mrc_train/images
    no_system_prompt: true

scienceqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/yunhaof/workspace/datasets/evaluation/scienceqa/scienceqa_train_12k.json
    media_dir: /home/yunhaof/workspace/datasets/evaluation/scienceqa/images

sharegpt4v-instruct-100k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/sharegpt4v/sharegpt4v_instruct_gpt4-vision_cap100k.json
    media_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets

sharegpt4v_gpt4_100k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/jason-filter-sharegpt4v_instruct_gpt4-vision_cap100k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/data

sharegpt4v_sft:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/jason-filter-sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/data

sharegpt_video:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/video_datasets_v2/sharegpt_video/video_caption_pretrain.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/video_datasets_v2/sharegpt_video/videos

sherlock:
    _target_: llava.data.LLaVADataset
    data_path: /home/yunhaof/workspace/datasets/sherlock/processed/sherlock_317k.json
    media_dir: /home/yunhaof/workspace/datasets/sherlock/images

shot2story_shotonly:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/shot2story/train-shortclip-processed-bin.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/shot2story/Shot2Story/data/videos_extracted

synthdog_en:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/synthdog_en.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/synthdog-en

text_flan_1m:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/text_flan_1m/data.json
    no_system_prompt: true

vatex:
    _target_: llava.data.LLaVADataset
    data_path: /home/jasonlu/video_datasets/jason_filtered_vatex.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/video_datasets_v2/vatex/videos_clipped

video_chatgpt:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/video_chatgpt/data.json
    media_dir: /home/jasonlu/video_datasets/Video_ChatGPT/activitynet_videos

vqa_activitynet-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_activitynet-qa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_activitynet-qa_train/images
    no_system_prompt: true

vqa_docvqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_docvqa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_docvqa_train/images
    no_system_prompt: true

vqa_gqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_gqa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_gqa_train/images
    no_system_prompt: true

vqa_ivqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_ivqa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_ivqa_train/images
    no_system_prompt: true

vqa_msrvtt-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_msrvtt-qa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_msrvtt-qa_train/images
    no_system_prompt: true

vqa_msvd-qa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_msvd-qa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_msvd-qa_train/images
    no_system_prompt: true

vqa_ocr-vqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_ocr-vqa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_ocr-vqa_train/images
    no_system_prompt: true

vqa_st-vqa_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_st-vqa_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_st-vqa_train/images
    no_system_prompt: true

vqa_viquae_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_viquae_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_viquae_train/images
    no_system_prompt: true

vqa_vqa-v2_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/zhijianl/workspace/datasets/vflan/vqa_vqa-v2_train/data.json
    media_dir: /home/zhijianl/workspace/datasets/vflan/vqa_vqa-v2_train/images
    no_system_prompt: true

warehouse-dvc:
    _target_: llava.data.DVCDataset
    data_path: /home/shijial/workspace/LITA-1.5/data/warehouse/train.json
    video_dir: /home/shijial/workspace/LITA-1.5/data/warehouse/videos

warehouse-el:
    _target_: llava.data.ELDataset
    data_path: /home/shijial/workspace/LITA-1.5/data/warehouse/train.json
    video_dir: /home/shijial/workspace/LITA-1.5/data/warehouse/videos

wit_subset:
    _target_: llava.data.LLaVADataset
    data_path: /home/yunhaof/workspace/datasets/WIT/wit_1_8m/wit_processed_538k.json
    media_dir: /home/yunhaof/workspace/datasets/WIT/wit_1_8m/images

youcook2-dvc:
    _target_: llava.data.DVCDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/VidChapters/YouCook2/train.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/youcook2/youcook2_frames

youcook2-el:
    _target_: llava.data.ELDataset
    data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/VidChapters/YouCook2/train.json
    video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/youcook2/youcook2_frames

youcook2:
    _target_: llava.data.LLaVADataset
    data_path: /home/jasonlu/video_datasets/jason_filtered_youcook2.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/video_datasets_v2/youcook2/video_data_clipped

docmatix_750k:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_750k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix

docmatix_2:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_750000-1000000.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix

docmatix_3:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix/conversations_1000000-1250000.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/Docmatix

doc_reason:
    _target_: llava.data.LLaVADataset
    data_path: /home/haotiant/workspace/dataset/vila_data/DocReason25K/docreason25k_processed.jsonl
    media_dir: /home/haotiant/workspace/dataset/vila_data/DocReason25K

kvqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/haotiant/workspace/dataset/vila_data/kvqa/raw/kvqa_processed.json
    media_dir: /home/haotiant/workspace/dataset/vila_data/kvqa/raw

metamathqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/haotiant/workspace/dataset/vila_data/MetaMathQA/MetaMathQA-395K_processed.json

mminstruct:
    _target_: llava.data.LLaVADataset
    data_path: /home/haotiant/dataset/vila_data/MMInstruct/jsons_all/qa_en.jsonl
    media_dir: /home/haotiant/dataset/vila_data/MMInstruct/images

unichart:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/unichart-qa-data/unichart-qa_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/unichart-qa-data/images

mtwi:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/mtwi_train/mtwi_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/mtwi_train/image_train

art:
    _target_: llava.data.LLaVADataset
    data_path: /home/haotiant/dataset/vila_data/art_shangy/art500k_processed.jsonl
    media_dir: /home/haotiant/dataset/vila_data/art_shangy

art1:
    _target_: llava.data.LLaVADataset
    data_path: //lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ART/ART_processed1.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ART/train_images

art2:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ART/ART_processed2.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ART/train_task2_images

lsvt:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/LSVT/LSVT_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/LSVT/train_full_images

unichart-pretrain:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/unichart-pretrain-data/unichart-pretrain_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/UniChart-pretrain-images/content/images

unichart-pretrain-1m:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/unichart-pretrain-data/unichart-pretrain_processed_1m.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/UniChart-pretrain-images/content/images

rects:
    _target_: llava.data.LLaVADataset
    data_path: //lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ReCTS/ReCTS_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ReCTS/img

koniq:
    _target_: llava.data.LLaVADataset
    data_path: /home/shangy/label/koniq10k/koniq10k.jsonl
    media_dir: /home/shangy/label/koniq10k/koniq10k

casia:
    _target_: llava.data.LLaVADataset
    data_path: /home/shangy/label/CASIA/CASIA2.0_revised/Au.jsonl
    media_dir: /home/shangy/label/CASIA/CASIA2.0_revised/Au

pdfa-1m:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/pdfa/vanilla_qa/pdfa_1m.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/pdfa/vanilla_qa

gsm8k_rendering:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/gsm8k/gsm8k_rendering.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/gsm8k/images

metamathqa_rendering:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/MetaMathQA/MetaMathQA_rendering.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/MetaMathQA/images

squad_rendering:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/squad/squad_rendering.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/squad/images

squad:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/squad/squad_converted.jsonl

sroie:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/sroie/SROIE_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/sroie/train

grandstaff_qa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/grandstaff/grandstaff_converted.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/grandstaff/images

pdfvqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/pdfvqa/pdfvqa_converted.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/pdfvqa/images

chembl:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/chembl/chembl_converted.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/chembl/images

orand-b:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ORAND-CAR-B_train.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/images/ORAND-CAR-B

dtvqa-prod:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/metadata/DT-VQA-DTProdCap_train.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/images/DT-VQA-DTProdCap

dtvqa-scene:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/metadata/DT-VQA-DTScene_train.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/images/DT-VQA-DTScene

dtvqa-tab:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/metadata/DT-VQA-DTTabShot_train.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/yuxian_data/images/DT-VQA-DTTabShot

ureaderkg:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ureader_kg/ureader_kg_processed.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ureader_kg

wordart:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/WordArt_train_label_out_processed.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/WordArt

pathvqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/pathvqa/pvqa/train_32k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/pathvqa/pvqa/images

pmcvqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/pmc-vqa/train_176k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/pmc-vqa/images

medvqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/medvqa/train_data_17k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/medvqa/medvqa/data/

slake:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/slake/slake_train_val_instruct_high_freq_4.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/slake/imgs

mimicvqa:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/MIMIC_VQA/all_images_json/llava_med_instruct_mimicvqa_train.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/MIMIC_VQA/images_small

llave_onevision_images_sft_cot2:
    _target_: llava.data.LLaVACOTDataset
    data_path: /home/haotiant/dataset/vila_data/LLaVA-OneVision-Data-processed/llava_onevision_sft_images_only_non_repeat_train.jsonl
    media_dir: /home/haotiant/dataset/vila_data/LLaVA-OneVision-Data-processed/images
    name: llave_onevision_images_sft
    cot_relabel_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/cot_llava_1021fixed_processed.json

sharegpt4v_sft_cot2:
    _target_: llava.data.LLaVACOTDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/jason-filter-sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/data
    name: sharegpt4v_sft
    cot_relabel_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/cot_llava_1021fixed_processed.json

geoqa_cot2:
    _target_: llava.data.LLaVACOTDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/geoqa+.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/geoqa+
    name: geoqa
    cot_relabel_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/cot_llava_1021fixed_processed.json

chartqa_train_18k_cot2:
    _target_: llava.data.LLaVACOTDataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/chartqa_train_18k.jsonl
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/chartqa
    name: chartqa_train_18k
    cot_relabel_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/cot_llava_1021fixed_processed.json

# 154,608
olmo_pointing:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 27,856
olmo_pointing_qa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing_qa.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 65,934
olmo_pointing_high_freq:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/pointing_high_frequency.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 46,518
olmo_doc_table:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_tables.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 71,282
olmo_doc_doc:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_documents.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 16,551
olmo_doc_diagrams:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_diagrams.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 116,814
olmo_doc_charts:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/documents_charts.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 203,559
olmo_clock200k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/clocks_processed_200k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 71,172
olmo_askanything:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/ask_model_anything.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo
# 162,855
olmo_capqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo/llava/capqa.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/pixmo

llava_1_5_mm_align:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/LLaVA-CC3M-Pretrain-595K/chat.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/LLaVA-CC3M-Pretrain-595K/images

allava_caption_vflan:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/ALLaVA-4V/ALLaVA-Caption-VFLAN-4V.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-pretrain/ALLaVA-4V/

sharegpt4v_pretrain:
    _target_: llava.data.LLaVADataset
    data_path: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/jason-filter-share-captioner_coco_lcs_sam_1246k_1107.json
    media_dir: /lustre/fsw/portfolios/nvr/projects/nvr_elm_llm/dataset/vila-sft/ShareGPT4V/data
