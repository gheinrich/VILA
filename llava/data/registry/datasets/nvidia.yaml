---
dummy:
    _target_: llava.data.DummyDataset
    num_instances: 10000
    comments: dummy dataset for testing

llave_onevision_images_sft_cot:
    _target_: llava.data.LLaVACOTDataset
    data_path: /home/haotiant/dataset/vila_data/LLaVA-OneVision-Data-processed/llava_onevision_sft_images_only_non_repeat_train.jsonl
    media_dir: /home/haotiant/dataset/vila_data/LLaVA-OneVision-Data-processed/images
    name: llave_onevision_images_sft
    cot_relabel_path: /home/ligengz/workspace/VILA-dev/data_curation_dev/recaptioned_cot_llava.json

sharegpt4v_sft_cot:
    _target_: llava.data.LLaVACOTDataset
    data_path: /home/jasonlu/vlm_datasets/ShareGPT4V/jason-filter-sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json
    media_dir: /home/jasonlu/vlm_datasets/ShareGPT4V/data
    name: sharegpt4v_sft
    cot_relabel_path: /home/ligengz/workspace/VILA-dev/data_curation_dev/recaptioned_cot_llava.json

geoqa_cot:
    _target_: llava.data.LLaVACOTDataset
    data_path: /home/jasonlu/workspace/InternVL/internvl_chat/playground/geoqa+.jsonl
    media_dir: /home/jasonlu/workspace/InternVL/internvl_chat/playground/data/geoqa+
    name: geoqa
    cot_relabel_path: /home/ligengz/workspace/VILA-dev/data_curation_dev/recaptioned_cot_llava.json

chartqa_train_18k_cot:
    _target_: llava.data.LLaVACOTDataset
    data_path: /home/jasonlu/workspace/InternVL/internvl_chat/playground/chartqa_train_18k.jsonl
    media_dir: /home/jasonlu/workspace/InternVL/internvl_chat/playground/data/chartqa
    name: chartqa_train_18k
    cot_relabel_path: /home/ligengz/workspace/VILA-dev/data_curation_dev/recaptioned_cot_llava.json

tabmwp_cot:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/PromptPG/data/tabmwp/problems_train_vila_cot.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/PromptPG/data/tabmwp

tabmwp:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/PromptPG/data/tabmwp/problems_train_vila.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/PromptPG/data/tabmwp

mmc_instruction:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/MMC-Instruction/processed/mmc_instruction_410k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/MMC-Instruction

nextqa_mc:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/train-processed-filtered-mc.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/NExTQA/test

mmbench_val:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/MMBench/data/mmbench_4k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/MMBench/images

vcg_human:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/annotations/vcg_human_annotated.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/Activity_Videos

unimm_chat:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/UniMM-Chat/UniMM-Chat_117k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/coco

textocr_qa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/textocr-gpt4v/processed/textocr_25k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/TextOCR/train_images

svit:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/SVIT/SVIT_108k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/svit

stem_qa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/STEM/processed/stem_sft_644k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/STEM/processed/images/train

ssv2:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/annotations/classification_ssv2_converted.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/ssv2

shikra:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/shikra/shikra.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/flickr30k-images

# coco_clip and coco?
refcoco_train:
    _target_: llava.data.LLaVADataset
    data_path: /home/yunhaof/workspace/datasets/grounding/annotations/processed/refcoco_train.json
    media_dir: /home/yunhaof/workspace/datasets

reason_clevrerqa:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/annotations/reasoning_clevrer_qa_converted.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/clevrer

reason_clevrermc:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/annotations/reasoning_clevrer_mc_converted.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/clevrer

nv_mm_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/nv_mm_sft/nv_mm_sft.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/nv_mm_sft/images

k710:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/annotations/classification_k710_converted.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/VideoGPT-plus_Training_Dataset/instruction_tuning/k710


llave_onevision_images_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/llava_onevision_sft_images_only_non_repeat_train.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/images

llave_onevision_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/llava_onevision_sft_non_repeat_train.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/LLaVA-OneVision-Data-processed/images

idefics2_sft:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/idefics2-sft/processed/idefics2_sft_w_table.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/idefics2-sft

cambrian_1375k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/cambrian/cambrian_1375k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/Cambrian/data

cvbench:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/CV-Bench/test_2k.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/CV-Bench

ai2d_train_12k:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/ai2d_train_12k.jsonl
    media_dir: /home/ligengz/nvr_elm_llm/dataset/vila-sft/internvl_chat/playground/data/ai2d

shot2story_shotonly:
    _target_: llava.data.LLaVADataset
    data_path: /home/ligengz/nvr_elm_llm/dataset/shot2story/train-shortclip-processed-bin.json
    media_dir: /home/ligengz/nvr_elm_llm/dataset/shot2story/Shot2Story/data/videos_extracted
