llava-instruct-150k:
  _target_: llava.data.LLaVADataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/LLaVA-Instruct-150K/llava_v1_5_mix665k.json
  image_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets

sharegpt4v-instruct-100k:
  _target_: llava.data.LLaVADataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/sharegpt4v/sharegpt4v_instruct_gpt4-vision_cap100k.json
  image_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets

activitynet-dvc:
  _target_: llava.data.DVCDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/train.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

youcook2-dvc:
  _target_: llava.data.DVCDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/VidChapters/YouCook2/train.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/youcook2/youcook2_frames

medical-dvc:
  _target_: llava.data.DVCDataset
  data_path: /home/shijial/workspace/LITA-1.5/data/medical/train.json
  video_dir: /home/shijial/workspace/LITA-1.5/data/medical/videos

warehouse-dvc:
  _target_: llava.data.DVCDataset
  data_path: /home/shijial/workspace/LITA-1.5/data/warehouse/train.json
  video_dir: /home/shijial/workspace/LITA-1.5/data/warehouse/videos

activitynet-el:
  _target_: llava.data.ELDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/train.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

charades-el:
  _target_: llava.data.ELDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/charades/train_processed.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/charades/Charades_v1_480

didemo-el:
  _target_: llava.data.ELDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/didemo/train_processed.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/didemo/train_videos_480p

medical-el:
  _target_: llava.data.ELDataset
  data_path: /home/shijial/workspace/LITA-1.5/data/medical/train.json
  video_dir: /home/shijial/workspace/LITA-1.5/data/medical/videos

warehouse-el:
  _target_: llava.data.ELDataset
  data_path: /home/shijial/workspace/LITA-1.5/data/warehouse/train.json
  video_dir: /home/shijial/workspace/LITA-1.5/data/warehouse/videos

youcook2-el:
  _target_: llava.data.ELDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/VidChapters/YouCook2/train.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/youcook2/youcook2_frames

nextqa:
  _target_: llava.data.VideoQADataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/nextqa/train_processed.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/nextqa/NExTVideo
  task_prompt: "Answer the question using a short phrase."

activitynet-rtl:
  _target_: llava.data.RTLDataset
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/temporal_reasoning/activitynet_train_gpt-4-0613_temp_6_f10009.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

activitynet-rtl/val:
  data_path: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/temporal_reasoning/annot_val_1_q229.json
  video_dir: /lustre/fsw/portfolios/nvr/users/deahuang/datasets/activitynet-captions/activitynet_frames

mint-1t-arxiv-captions:
  _target_: llava.data.LLaVADataset
  data_path: /home/zhijianl/workspace/datasets/mint-1t-arxiv/captions-20240904.json
  image_dir: /home/zhijianl/workspace/datasets/mint-1t-arxiv/tiff
